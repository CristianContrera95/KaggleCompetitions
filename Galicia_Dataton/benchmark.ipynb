{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CONTENT_CATEGORY_BOTTOM.csv', 'CONTENT_CATEGORY_TOP.csv', 'PAGE.csv', 'pageviews.csv', 'conversiones.csv', 'device_data.csv', 'CONTENT_CATEGORY.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"input/\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "662"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.stat('input/pageviews.csv').st_size // (1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/who/Enviroments/kaggle-env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-7dfa5ccabfe7>\", line 1, in <module>\n",
      "    df = pd.read_csv('input/pageviews.csv', parse_dates=[\"FEC_EVENT\"])\n",
      "  File \"/home/who/Enviroments/kaggle-env/lib/python3.6/site-packages/pandas/io/parsers.py\", line 685, in parser_f\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/home/who/Enviroments/kaggle-env/lib/python3.6/site-packages/pandas/io/parsers.py\", line 463, in _read\n",
      "    data = parser.read(nrows)\n",
      "  File \"/home/who/Enviroments/kaggle-env/lib/python3.6/site-packages/pandas/io/parsers.py\", line 1154, in read\n",
      "    ret = self._engine.read(nrows)\n",
      "  File \"/home/who/Enviroments/kaggle-env/lib/python3.6/site-packages/pandas/io/parsers.py\", line 2048, in read\n",
      "    data = self._reader.read(nrows)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 879, in pandas._libs.parsers.TextReader.read\n",
      "  File \"pandas/_libs/parsers.pyx\", line 894, in pandas._libs.parsers.TextReader._read_low_memory\n",
      "  File \"pandas/_libs/parsers.pyx\", line 971, in pandas._libs.parsers.TextReader._read_rows\n",
      "  File \"pandas/_libs/parsers.pyx\", line 1103, in pandas._libs.parsers.TextReader._convert_column_data\n",
      "  File \"pandas/_libs/parsers.pyx\", line 1149, in pandas._libs.parsers.TextReader._convert_tokens\n",
      "  File \"pandas/_libs/parsers.pyx\", line 1195, in pandas._libs.parsers.TextReader._convert_with_dtype\n",
      "  File \"/home/who/Enviroments/kaggle-env/lib/python3.6/site-packages/pandas/core/dtypes/common.py\", line 678, in is_categorical_dtype\n",
      "    def is_categorical_dtype(arr_or_dtype):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/who/Enviroments/kaggle-env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/who/Enviroments/kaggle-env/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/who/Enviroments/kaggle-env/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/who/Enviroments/kaggle-env/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/home/who/Enviroments/kaggle-env/lib/python3.6/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('input/pageviews.csv', parse_dates=[\"FEC_EVENT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEC_EVENT</th>\n",
       "      <th>PAGE</th>\n",
       "      <th>CONTENT_CATEGORY</th>\n",
       "      <th>CONTENT_CATEGORY_TOP</th>\n",
       "      <th>CONTENT_CATEGORY_BOTTOM</th>\n",
       "      <th>SITE_ID</th>\n",
       "      <th>ON_SITE_SEARCH_TERM</th>\n",
       "      <th>USER_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-30 07:35:48</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-03-30 07:35:52</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-03-30 07:36:11</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-03-30 07:36:16</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-03-30 07:41:38</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-03-30 07:41:42</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-03-30 07:42:01</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-03-30 07:42:05</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-03-30 07:43:43</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-03-30 07:44:14</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-03-30 07:44:37</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-03-30 07:44:58</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018-02-18 13:26:45</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-02-05 14:39:37</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018-03-08 10:51:34</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018-03-08 10:51:37</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018-03-08 10:52:05</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-03-08 10:52:06</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-03-08 10:52:06</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-03-08 10:52:12</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018-01-24 12:24:29</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018-01-24 12:24:49</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2018-01-24 12:24:50</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2018-01-24 12:24:50</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2018-03-04 18:04:12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2018-01-04 08:29:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2018-01-04 08:42:20</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2018-01-04 08:42:21</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2018-01-04 08:42:38</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2018-01-04 08:42:38</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936904</th>\n",
       "      <td>2018-09-17 13:41:36</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936905</th>\n",
       "      <td>2018-09-17 13:41:41</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936906</th>\n",
       "      <td>2018-09-17 13:42:01</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936907</th>\n",
       "      <td>2018-09-17 13:43:29</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936908</th>\n",
       "      <td>2018-09-17 13:48:32</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936909</th>\n",
       "      <td>2018-10-07 10:12:21</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936910</th>\n",
       "      <td>2018-10-07 10:12:32</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936911</th>\n",
       "      <td>2018-10-07 10:12:50</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936912</th>\n",
       "      <td>2018-10-07 10:12:52</td>\n",
       "      <td>190</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936913</th>\n",
       "      <td>2018-10-07 10:12:59</td>\n",
       "      <td>190</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936914</th>\n",
       "      <td>2018-10-07 10:13:01</td>\n",
       "      <td>190</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936915</th>\n",
       "      <td>2018-10-07 10:13:04</td>\n",
       "      <td>280</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936916</th>\n",
       "      <td>2018-10-07 10:13:19</td>\n",
       "      <td>190</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936917</th>\n",
       "      <td>2018-10-07 10:18:21</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936918</th>\n",
       "      <td>2018-09-17 13:40:49</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936919</th>\n",
       "      <td>2018-07-05 13:25:02</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936920</th>\n",
       "      <td>2018-07-05 13:25:22</td>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936921</th>\n",
       "      <td>2018-07-05 13:25:34</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936922</th>\n",
       "      <td>2018-07-05 13:26:34</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936923</th>\n",
       "      <td>2018-07-05 13:27:01</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936924</th>\n",
       "      <td>2018-07-05 13:28:03</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936925</th>\n",
       "      <td>2018-07-05 13:28:22</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936926</th>\n",
       "      <td>2018-07-05 13:33:24</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936927</th>\n",
       "      <td>2018-10-31 16:16:04</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936928</th>\n",
       "      <td>2018-10-31 16:17:46</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936929</th>\n",
       "      <td>2018-10-31 16:18:06</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936930</th>\n",
       "      <td>2018-10-31 16:18:35</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936931</th>\n",
       "      <td>2018-10-31 16:23:38</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936932</th>\n",
       "      <td>2018-10-16 10:53:29</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936933</th>\n",
       "      <td>2018-10-16 10:54:09</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17936934 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   FEC_EVENT  PAGE   ...     ON_SITE_SEARCH_TERM  USER_ID\n",
       "0        2018-03-30 07:35:48     1   ...                       1        0\n",
       "1        2018-03-30 07:35:52     2   ...                       1        0\n",
       "2        2018-03-30 07:36:11     3   ...                       1        0\n",
       "3        2018-03-30 07:36:16     4   ...                       1        0\n",
       "4        2018-03-30 07:41:38     5   ...                       1        0\n",
       "5        2018-03-30 07:41:42     2   ...                       1        0\n",
       "6        2018-03-30 07:42:01     3   ...                       1        0\n",
       "7        2018-03-30 07:42:05     4   ...                       1        0\n",
       "8        2018-03-30 07:43:43     3   ...                       1        0\n",
       "9        2018-03-30 07:44:14     6   ...                       1        0\n",
       "10       2018-03-30 07:44:37     7   ...                       1        0\n",
       "11       2018-03-30 07:44:58     2   ...                       1        0\n",
       "12       2018-02-18 13:26:45     1   ...                       1        0\n",
       "13       2018-02-05 14:39:37     8   ...                       1        0\n",
       "14       2018-03-08 10:51:34     2   ...                       1        0\n",
       "15       2018-03-08 10:51:37     9   ...                       1        0\n",
       "16       2018-03-08 10:52:05    10   ...                       1        0\n",
       "17       2018-03-08 10:52:06    11   ...                       1        0\n",
       "18       2018-03-08 10:52:06    12   ...                       1        0\n",
       "19       2018-03-08 10:52:12    13   ...                       1        0\n",
       "20       2018-01-24 12:24:29    14   ...                       1        0\n",
       "21       2018-01-24 12:24:49    10   ...                       1        0\n",
       "22       2018-01-24 12:24:50    11   ...                       1        0\n",
       "23       2018-01-24 12:24:50    12   ...                       1        0\n",
       "24       2018-03-04 18:04:12     1   ...                       1        0\n",
       "25       2018-01-04 08:29:59     1   ...                       1        0\n",
       "26       2018-01-04 08:42:20     2   ...                       1        0\n",
       "27       2018-01-04 08:42:21    14   ...                       1        0\n",
       "28       2018-01-04 08:42:38    11   ...                       1        0\n",
       "29       2018-01-04 08:42:38    12   ...                       1        0\n",
       "...                      ...   ...   ...                     ...      ...\n",
       "17936904 2018-09-17 13:41:36     3   ...                       1     4639\n",
       "17936905 2018-09-17 13:41:41    64   ...                       1     4639\n",
       "17936906 2018-09-17 13:42:01     3   ...                       1     4639\n",
       "17936907 2018-09-17 13:43:29    42   ...                       1     4639\n",
       "17936908 2018-09-17 13:48:32     5   ...                       1     4639\n",
       "17936909 2018-10-07 10:12:21     2   ...                       1     4639\n",
       "17936910 2018-10-07 10:12:32     3   ...                       1     4639\n",
       "17936911 2018-10-07 10:12:50    39   ...                       1     4639\n",
       "17936912 2018-10-07 10:12:52   190   ...                       1     4639\n",
       "17936913 2018-10-07 10:12:59   190   ...                       1     4639\n",
       "17936914 2018-10-07 10:13:01   190   ...                       1     4639\n",
       "17936915 2018-10-07 10:13:04   280   ...                       1     4639\n",
       "17936916 2018-10-07 10:13:19   190   ...                       1     4639\n",
       "17936917 2018-10-07 10:18:21     5   ...                       1     4639\n",
       "17936918 2018-09-17 13:40:49    40   ...                       1     4639\n",
       "17936919 2018-07-05 13:25:02     2   ...                       1     4639\n",
       "17936920 2018-07-05 13:25:22    99   ...                       1     4639\n",
       "17936921 2018-07-05 13:25:34     2   ...                       1     4639\n",
       "17936922 2018-07-05 13:26:34     3   ...                       1     4639\n",
       "17936923 2018-07-05 13:27:01     4   ...                       1     4639\n",
       "17936924 2018-07-05 13:28:03     3   ...                       1     4639\n",
       "17936925 2018-07-05 13:28:22     4   ...                       1     4639\n",
       "17936926 2018-07-05 13:33:24     5   ...                       1     4639\n",
       "17936927 2018-10-31 16:16:04    40   ...                       1     4639\n",
       "17936928 2018-10-31 16:17:46     2   ...                       1     4639\n",
       "17936929 2018-10-31 16:18:06     3   ...                       1     4639\n",
       "17936930 2018-10-31 16:18:35    23   ...                       1     4639\n",
       "17936931 2018-10-31 16:23:38     5   ...                       1     4639\n",
       "17936932 2018-10-16 10:53:29     1   ...                       1     4639\n",
       "17936933 2018-10-16 10:54:09     2   ...                       1     4639\n",
       "\n",
       "[17936934 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../input/pageviews.csv\",\n",
    "                   parse_dates=[\"FEC_EVENT\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haciendo PAGE\n",
      "haciendo CONTENT_CATEGORY\n",
      "haciendo CONTENT_CATEGORY_TOP\n",
      "haciendo CONTENT_CATEGORY_BOTTOM\n",
      "haciendo SITE_ID\n",
      "haciendo ON_SITE_SEARCH_TERM\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "for c in data.drop([\"USER_ID\", \"FEC_EVENT\"], axis=1).columns:\n",
    "    print(\"haciendo\", c)\n",
    "    temp = pd.crosstab(data.USER_ID, data[c])\n",
    "    temp.columns = [c + \"_\" + str(v) for v in temp.columns]\n",
    "    X_test.append(temp.apply(lambda x: x / x.sum(), axis=1))\n",
    "X_test = pd.concat(X_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haciendo PAGE\n",
      "haciendo CONTENT_CATEGORY\n",
      "haciendo CONTENT_CATEGORY_TOP\n",
      "haciendo CONTENT_CATEGORY_BOTTOM\n",
      "haciendo SITE_ID\n",
      "haciendo ON_SITE_SEARCH_TERM\n"
     ]
    }
   ],
   "source": [
    "data = data[data.FEC_EVENT.dt.month < 10]\n",
    "X_train = []\n",
    "for c in data.drop([\"USER_ID\", \"FEC_EVENT\"], axis=1).columns:\n",
    "    print(\"haciendo\", c)\n",
    "    temp = pd.crosstab(data.USER_ID, data[c])\n",
    "    temp.columns = [c + \"_\" + str(v) for v in temp.columns]\n",
    "    X_train.append(temp.apply(lambda x: x / x.sum(), axis=1))\n",
    "X_train = pd.concat(X_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(set(X_train.columns).intersection(set(X_test.columns)))\n",
    "X_train = X_train[features]\n",
    "X_test = X_test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prev = pd.read_csv(\"../input/conversiones/conversiones.csv\")\n",
    "y_train = pd.Series(0, index=X_train.index)\n",
    "idx = set(y_prev[y_prev.mes >= 10].USER_ID.unique()).intersection(\n",
    "        set(X_train.index))\n",
    "y_train.loc[list(idx)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's auc: 0.811872\ttraining's binary_logloss: 0.131954\tvalid_1's auc: 0.764508\tvalid_1's binary_logloss: 0.138962\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's auc: 0.859237\ttraining's binary_logloss: 0.122981\tvalid_1's auc: 0.806205\tvalid_1's binary_logloss: 0.136569\n",
      "[3]\ttraining's auc: 0.876797\ttraining's binary_logloss: 0.115215\tvalid_1's auc: 0.821108\tvalid_1's binary_logloss: 0.134311\n",
      "[4]\ttraining's auc: 0.895254\ttraining's binary_logloss: 0.109165\tvalid_1's auc: 0.812754\tvalid_1's binary_logloss: 0.133995\n",
      "[5]\ttraining's auc: 0.903618\ttraining's binary_logloss: 0.104088\tvalid_1's auc: 0.82676\tvalid_1's binary_logloss: 0.132602\n",
      "[6]\ttraining's auc: 0.915003\ttraining's binary_logloss: 0.0992827\tvalid_1's auc: 0.82002\tvalid_1's binary_logloss: 0.132049\n",
      "[7]\ttraining's auc: 0.920607\ttraining's binary_logloss: 0.095195\tvalid_1's auc: 0.818992\tvalid_1's binary_logloss: 0.13058\n",
      "[8]\ttraining's auc: 0.937706\ttraining's binary_logloss: 0.0915424\tvalid_1's auc: 0.811738\tvalid_1's binary_logloss: 0.130395\n",
      "[9]\ttraining's auc: 0.94185\ttraining's binary_logloss: 0.0882886\tvalid_1's auc: 0.807866\tvalid_1's binary_logloss: 0.129939\n",
      "[10]\ttraining's auc: 0.94637\ttraining's binary_logloss: 0.0850558\tvalid_1's auc: 0.806611\tvalid_1's binary_logloss: 0.129767\n",
      "[11]\ttraining's auc: 0.957325\ttraining's binary_logloss: 0.081904\tvalid_1's auc: 0.800791\tvalid_1's binary_logloss: 0.129575\n",
      "[12]\ttraining's auc: 0.964329\ttraining's binary_logloss: 0.0794097\tvalid_1's auc: 0.799297\tvalid_1's binary_logloss: 0.129676\n",
      "[13]\ttraining's auc: 0.966655\ttraining's binary_logloss: 0.0768521\tvalid_1's auc: 0.799118\tvalid_1's binary_logloss: 0.129101\n",
      "[14]\ttraining's auc: 0.971765\ttraining's binary_logloss: 0.0740624\tvalid_1's auc: 0.79527\tvalid_1's binary_logloss: 0.129355\n",
      "[15]\ttraining's auc: 0.977832\ttraining's binary_logloss: 0.0715956\tvalid_1's auc: 0.795019\tvalid_1's binary_logloss: 0.129744\n",
      "Early stopping, best iteration is:\n",
      "[5]\ttraining's auc: 0.903618\ttraining's binary_logloss: 0.104088\tvalid_1's auc: 0.82676\tvalid_1's binary_logloss: 0.132602\n",
      "[1]\ttraining's auc: 0.839935\ttraining's binary_logloss: 0.131966\tvalid_1's auc: 0.762591\tvalid_1's binary_logloss: 0.145048\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's auc: 0.874033\ttraining's binary_logloss: 0.121999\tvalid_1's auc: 0.822975\tvalid_1's binary_logloss: 0.140064\n",
      "[3]\ttraining's auc: 0.893593\ttraining's binary_logloss: 0.11464\tvalid_1's auc: 0.833189\tvalid_1's binary_logloss: 0.136311\n",
      "[4]\ttraining's auc: 0.913739\ttraining's binary_logloss: 0.10871\tvalid_1's auc: 0.838376\tvalid_1's binary_logloss: 0.133219\n",
      "[5]\ttraining's auc: 0.921946\ttraining's binary_logloss: 0.103917\tvalid_1's auc: 0.831779\tvalid_1's binary_logloss: 0.131842\n",
      "[6]\ttraining's auc: 0.925579\ttraining's binary_logloss: 0.0992913\tvalid_1's auc: 0.83146\tvalid_1's binary_logloss: 0.130845\n",
      "[7]\ttraining's auc: 0.930394\ttraining's binary_logloss: 0.0952195\tvalid_1's auc: 0.838649\tvalid_1's binary_logloss: 0.129474\n",
      "[8]\ttraining's auc: 0.938886\ttraining's binary_logloss: 0.0915873\tvalid_1's auc: 0.840253\tvalid_1's binary_logloss: 0.128044\n",
      "[9]\ttraining's auc: 0.94308\ttraining's binary_logloss: 0.0885445\tvalid_1's auc: 0.838046\tvalid_1's binary_logloss: 0.127652\n",
      "[10]\ttraining's auc: 0.946818\ttraining's binary_logloss: 0.0854787\tvalid_1's auc: 0.843164\tvalid_1's binary_logloss: 0.126654\n",
      "[11]\ttraining's auc: 0.950062\ttraining's binary_logloss: 0.0826105\tvalid_1's auc: 0.842641\tvalid_1's binary_logloss: 0.126336\n",
      "[12]\ttraining's auc: 0.953807\ttraining's binary_logloss: 0.0800233\tvalid_1's auc: 0.840798\tvalid_1's binary_logloss: 0.126229\n",
      "[13]\ttraining's auc: 0.96219\ttraining's binary_logloss: 0.0773683\tvalid_1's auc: 0.841367\tvalid_1's binary_logloss: 0.126027\n",
      "[14]\ttraining's auc: 0.968544\ttraining's binary_logloss: 0.0746961\tvalid_1's auc: 0.840821\tvalid_1's binary_logloss: 0.12579\n",
      "[15]\ttraining's auc: 0.971113\ttraining's binary_logloss: 0.0723608\tvalid_1's auc: 0.843369\tvalid_1's binary_logloss: 0.125034\n",
      "[16]\ttraining's auc: 0.975429\ttraining's binary_logloss: 0.0701115\tvalid_1's auc: 0.841549\tvalid_1's binary_logloss: 0.124853\n",
      "[17]\ttraining's auc: 0.980407\ttraining's binary_logloss: 0.0679171\tvalid_1's auc: 0.84578\tvalid_1's binary_logloss: 0.124827\n",
      "[18]\ttraining's auc: 0.987165\ttraining's binary_logloss: 0.0656352\tvalid_1's auc: 0.843267\tvalid_1's binary_logloss: 0.125385\n",
      "[19]\ttraining's auc: 0.989676\ttraining's binary_logloss: 0.0636271\tvalid_1's auc: 0.843312\tvalid_1's binary_logloss: 0.125165\n",
      "[20]\ttraining's auc: 0.991732\ttraining's binary_logloss: 0.0617947\tvalid_1's auc: 0.84578\tvalid_1's binary_logloss: 0.124669\n",
      "[21]\ttraining's auc: 0.993183\ttraining's binary_logloss: 0.0599643\tvalid_1's auc: 0.844097\tvalid_1's binary_logloss: 0.124572\n",
      "[22]\ttraining's auc: 0.99496\ttraining's binary_logloss: 0.0582293\tvalid_1's auc: 0.844552\tvalid_1's binary_logloss: 0.124756\n",
      "[23]\ttraining's auc: 0.995978\ttraining's binary_logloss: 0.0566301\tvalid_1's auc: 0.843904\tvalid_1's binary_logloss: 0.124525\n",
      "[24]\ttraining's auc: 0.997053\ttraining's binary_logloss: 0.0549132\tvalid_1's auc: 0.843244\tvalid_1's binary_logloss: 0.124521\n",
      "[25]\ttraining's auc: 0.997948\ttraining's binary_logloss: 0.0533962\tvalid_1's auc: 0.84429\tvalid_1's binary_logloss: 0.124144\n",
      "[26]\ttraining's auc: 0.99826\ttraining's binary_logloss: 0.0520035\tvalid_1's auc: 0.843631\tvalid_1's binary_logloss: 0.124445\n",
      "[27]\ttraining's auc: 0.998769\ttraining's binary_logloss: 0.0506915\tvalid_1's auc: 0.845064\tvalid_1's binary_logloss: 0.124172\n",
      "Early stopping, best iteration is:\n",
      "[17]\ttraining's auc: 0.980407\ttraining's binary_logloss: 0.0679171\tvalid_1's auc: 0.84578\tvalid_1's binary_logloss: 0.124827\n",
      "[1]\ttraining's auc: 0.846775\ttraining's binary_logloss: 0.130584\tvalid_1's auc: 0.781621\tvalid_1's binary_logloss: 0.147293\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's auc: 0.877104\ttraining's binary_logloss: 0.121\tvalid_1's auc: 0.799725\tvalid_1's binary_logloss: 0.143762\n",
      "[3]\ttraining's auc: 0.895043\ttraining's binary_logloss: 0.113368\tvalid_1's auc: 0.80881\tvalid_1's binary_logloss: 0.14172\n",
      "[4]\ttraining's auc: 0.902571\ttraining's binary_logloss: 0.107663\tvalid_1's auc: 0.819717\tvalid_1's binary_logloss: 0.139069\n",
      "[5]\ttraining's auc: 0.915142\ttraining's binary_logloss: 0.103028\tvalid_1's auc: 0.821638\tvalid_1's binary_logloss: 0.137179\n",
      "[6]\ttraining's auc: 0.925983\ttraining's binary_logloss: 0.0982167\tvalid_1's auc: 0.821094\tvalid_1's binary_logloss: 0.136157\n",
      "[7]\ttraining's auc: 0.933167\ttraining's binary_logloss: 0.0944048\tvalid_1's auc: 0.81313\tvalid_1's binary_logloss: 0.135904\n",
      "[8]\ttraining's auc: 0.936861\ttraining's binary_logloss: 0.0906439\tvalid_1's auc: 0.809187\tvalid_1's binary_logloss: 0.135842\n",
      "[9]\ttraining's auc: 0.946029\ttraining's binary_logloss: 0.0871848\tvalid_1's auc: 0.818539\tvalid_1's binary_logloss: 0.134153\n",
      "[10]\ttraining's auc: 0.949434\ttraining's binary_logloss: 0.0842452\tvalid_1's auc: 0.817084\tvalid_1's binary_logloss: 0.133844\n",
      "[11]\ttraining's auc: 0.951211\ttraining's binary_logloss: 0.0815702\tvalid_1's auc: 0.815918\tvalid_1's binary_logloss: 0.134329\n",
      "[12]\ttraining's auc: 0.953015\ttraining's binary_logloss: 0.0789991\tvalid_1's auc: 0.814419\tvalid_1's binary_logloss: 0.133784\n",
      "[13]\ttraining's auc: 0.963433\ttraining's binary_logloss: 0.0758928\tvalid_1's auc: 0.826858\tvalid_1's binary_logloss: 0.133417\n",
      "[14]\ttraining's auc: 0.967228\ttraining's binary_logloss: 0.0733909\tvalid_1's auc: 0.825614\tvalid_1's binary_logloss: 0.133631\n",
      "[15]\ttraining's auc: 0.973912\ttraining's binary_logloss: 0.0708499\tvalid_1's auc: 0.817173\tvalid_1's binary_logloss: 0.134534\n",
      "[16]\ttraining's auc: 0.982212\ttraining's binary_logloss: 0.0683366\tvalid_1's auc: 0.82015\tvalid_1's binary_logloss: 0.133789\n",
      "[17]\ttraining's auc: 0.985418\ttraining's binary_logloss: 0.0663791\tvalid_1's auc: 0.823082\tvalid_1's binary_logloss: 0.13283\n",
      "[18]\ttraining's auc: 0.986542\ttraining's binary_logloss: 0.06458\tvalid_1's auc: 0.824315\tvalid_1's binary_logloss: 0.132821\n",
      "[19]\ttraining's auc: 0.987817\ttraining's binary_logloss: 0.0625709\tvalid_1's auc: 0.823626\tvalid_1's binary_logloss: 0.132936\n",
      "[20]\ttraining's auc: 0.989679\ttraining's binary_logloss: 0.060705\tvalid_1's auc: 0.820472\tvalid_1's binary_logloss: 0.133127\n",
      "[21]\ttraining's auc: 0.991645\ttraining's binary_logloss: 0.0588913\tvalid_1's auc: 0.82447\tvalid_1's binary_logloss: 0.132422\n",
      "[22]\ttraining's auc: 0.992397\ttraining's binary_logloss: 0.0571271\tvalid_1's auc: 0.829579\tvalid_1's binary_logloss: 0.13217\n",
      "[23]\ttraining's auc: 0.994823\ttraining's binary_logloss: 0.0553889\tvalid_1's auc: 0.826492\tvalid_1's binary_logloss: 0.132488\n",
      "[24]\ttraining's auc: 0.995829\ttraining's binary_logloss: 0.0538037\tvalid_1's auc: 0.824426\tvalid_1's binary_logloss: 0.133065\n",
      "[25]\ttraining's auc: 0.997259\ttraining's binary_logloss: 0.0523386\tvalid_1's auc: 0.824337\tvalid_1's binary_logloss: 0.132804\n",
      "[26]\ttraining's auc: 0.997814\ttraining's binary_logloss: 0.0509716\tvalid_1's auc: 0.825314\tvalid_1's binary_logloss: 0.132685\n",
      "[27]\ttraining's auc: 0.998542\ttraining's binary_logloss: 0.0497258\tvalid_1's auc: 0.82698\tvalid_1's binary_logloss: 0.132354\n",
      "[28]\ttraining's auc: 0.998998\ttraining's binary_logloss: 0.0483299\tvalid_1's auc: 0.828402\tvalid_1's binary_logloss: 0.132341\n",
      "[29]\ttraining's auc: 0.99921\ttraining's binary_logloss: 0.0470387\tvalid_1's auc: 0.830401\tvalid_1's binary_logloss: 0.132274\n",
      "[30]\ttraining's auc: 0.999447\ttraining's binary_logloss: 0.0457394\tvalid_1's auc: 0.830446\tvalid_1's binary_logloss: 0.132461\n",
      "[31]\ttraining's auc: 0.999596\ttraining's binary_logloss: 0.0445478\tvalid_1's auc: 0.829291\tvalid_1's binary_logloss: 0.132747\n",
      "[32]\ttraining's auc: 0.999717\ttraining's binary_logloss: 0.0435609\tvalid_1's auc: 0.830135\tvalid_1's binary_logloss: 0.133012\n",
      "Early stopping, best iteration is:\n",
      "[22]\ttraining's auc: 0.992397\ttraining's binary_logloss: 0.0571271\tvalid_1's auc: 0.829579\tvalid_1's binary_logloss: 0.13217\n",
      "[1]\ttraining's auc: 0.83239\ttraining's binary_logloss: 0.132253\tvalid_1's auc: 0.816224\tvalid_1's binary_logloss: 0.146454\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's auc: 0.874387\ttraining's binary_logloss: 0.122667\tvalid_1's auc: 0.816503\tvalid_1's binary_logloss: 0.143549\n",
      "[3]\ttraining's auc: 0.889284\ttraining's binary_logloss: 0.11548\tvalid_1's auc: 0.817646\tvalid_1's binary_logloss: 0.141169\n",
      "[4]\ttraining's auc: 0.90248\ttraining's binary_logloss: 0.109815\tvalid_1's auc: 0.823135\tvalid_1's binary_logloss: 0.137785\n",
      "[5]\ttraining's auc: 0.912263\ttraining's binary_logloss: 0.104869\tvalid_1's auc: 0.823392\tvalid_1's binary_logloss: 0.136101\n",
      "[6]\ttraining's auc: 0.919406\ttraining's binary_logloss: 0.100315\tvalid_1's auc: 0.826865\tvalid_1's binary_logloss: 0.134764\n",
      "[7]\ttraining's auc: 0.931809\ttraining's binary_logloss: 0.0961698\tvalid_1's auc: 0.824732\tvalid_1's binary_logloss: 0.134049\n",
      "[8]\ttraining's auc: 0.938172\ttraining's binary_logloss: 0.0924405\tvalid_1's auc: 0.819242\tvalid_1's binary_logloss: 0.133876\n",
      "[9]\ttraining's auc: 0.943096\ttraining's binary_logloss: 0.089072\tvalid_1's auc: 0.817145\tvalid_1's binary_logloss: 0.133094\n",
      "[10]\ttraining's auc: 0.958293\ttraining's binary_logloss: 0.0856495\tvalid_1's auc: 0.816853\tvalid_1's binary_logloss: 0.132511\n",
      "[11]\ttraining's auc: 0.965805\ttraining's binary_logloss: 0.0828272\tvalid_1's auc: 0.815816\tvalid_1's binary_logloss: 0.132766\n",
      "[12]\ttraining's auc: 0.967312\ttraining's binary_logloss: 0.0800568\tvalid_1's auc: 0.818345\tvalid_1's binary_logloss: 0.132466\n",
      "[13]\ttraining's auc: 0.972813\ttraining's binary_logloss: 0.0772554\tvalid_1's auc: 0.817599\tvalid_1's binary_logloss: 0.132217\n",
      "[14]\ttraining's auc: 0.978691\ttraining's binary_logloss: 0.0747281\tvalid_1's auc: 0.814312\tvalid_1's binary_logloss: 0.132153\n",
      "[15]\ttraining's auc: 0.98148\ttraining's binary_logloss: 0.072301\tvalid_1's auc: 0.816562\tvalid_1's binary_logloss: 0.131531\n",
      "[16]\ttraining's auc: 0.983574\ttraining's binary_logloss: 0.0700331\tvalid_1's auc: 0.814406\tvalid_1's binary_logloss: 0.131351\n",
      "Early stopping, best iteration is:\n",
      "[6]\ttraining's auc: 0.919406\ttraining's binary_logloss: 0.100315\tvalid_1's auc: 0.826865\tvalid_1's binary_logloss: 0.134764\n",
      "[1]\ttraining's auc: 0.844759\ttraining's binary_logloss: 0.132914\tvalid_1's auc: 0.746014\tvalid_1's binary_logloss: 0.135344\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's auc: 0.873055\ttraining's binary_logloss: 0.122584\tvalid_1's auc: 0.741951\tvalid_1's binary_logloss: 0.133233\n",
      "[3]\ttraining's auc: 0.882329\ttraining's binary_logloss: 0.114647\tvalid_1's auc: 0.733307\tvalid_1's binary_logloss: 0.133069\n",
      "[4]\ttraining's auc: 0.908698\ttraining's binary_logloss: 0.108773\tvalid_1's auc: 0.767288\tvalid_1's binary_logloss: 0.130984\n",
      "[5]\ttraining's auc: 0.914484\ttraining's binary_logloss: 0.10391\tvalid_1's auc: 0.770756\tvalid_1's binary_logloss: 0.130552\n",
      "[6]\ttraining's auc: 0.919955\ttraining's binary_logloss: 0.0992256\tvalid_1's auc: 0.774185\tvalid_1's binary_logloss: 0.129961\n",
      "[7]\ttraining's auc: 0.924165\ttraining's binary_logloss: 0.0951482\tvalid_1's auc: 0.773499\tvalid_1's binary_logloss: 0.129187\n",
      "[8]\ttraining's auc: 0.935724\ttraining's binary_logloss: 0.0916977\tvalid_1's auc: 0.7706\tvalid_1's binary_logloss: 0.128429\n",
      "[9]\ttraining's auc: 0.940229\ttraining's binary_logloss: 0.0883243\tvalid_1's auc: 0.773551\tvalid_1's binary_logloss: 0.127605\n",
      "[10]\ttraining's auc: 0.945915\ttraining's binary_logloss: 0.0854422\tvalid_1's auc: 0.779477\tvalid_1's binary_logloss: 0.126407\n",
      "[11]\ttraining's auc: 0.959009\ttraining's binary_logloss: 0.082078\tvalid_1's auc: 0.774366\tvalid_1's binary_logloss: 0.12647\n",
      "[12]\ttraining's auc: 0.963196\ttraining's binary_logloss: 0.0794096\tvalid_1's auc: 0.777536\tvalid_1's binary_logloss: 0.126225\n",
      "[13]\ttraining's auc: 0.969374\ttraining's binary_logloss: 0.0766112\tvalid_1's auc: 0.757505\tvalid_1's binary_logloss: 0.12632\n",
      "[14]\ttraining's auc: 0.973823\ttraining's binary_logloss: 0.0741085\tvalid_1's auc: 0.758747\tvalid_1's binary_logloss: 0.12666\n",
      "[15]\ttraining's auc: 0.978065\ttraining's binary_logloss: 0.0715019\tvalid_1's auc: 0.772943\tvalid_1's binary_logloss: 0.126193\n",
      "[16]\ttraining's auc: 0.979577\ttraining's binary_logloss: 0.0692251\tvalid_1's auc: 0.77082\tvalid_1's binary_logloss: 0.1261\n",
      "[17]\ttraining's auc: 0.984166\ttraining's binary_logloss: 0.0668699\tvalid_1's auc: 0.770083\tvalid_1's binary_logloss: 0.125984\n",
      "[18]\ttraining's auc: 0.988375\ttraining's binary_logloss: 0.0645954\tvalid_1's auc: 0.77082\tvalid_1's binary_logloss: 0.126116\n",
      "[19]\ttraining's auc: 0.990044\ttraining's binary_logloss: 0.0627275\tvalid_1's auc: 0.770665\tvalid_1's binary_logloss: 0.126081\n",
      "[20]\ttraining's auc: 0.991939\ttraining's binary_logloss: 0.0609481\tvalid_1's auc: 0.776113\tvalid_1's binary_logloss: 0.126409\n",
      "Early stopping, best iteration is:\n",
      "[10]\ttraining's auc: 0.945915\ttraining's binary_logloss: 0.0854422\tvalid_1's auc: 0.779477\tvalid_1's binary_logloss: 0.126407\n",
      "[1]\ttraining's auc: 0.834838\ttraining's binary_logloss: 0.131161\tvalid_1's auc: 0.757325\tvalid_1's binary_logloss: 0.142823\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's auc: 0.877981\ttraining's binary_logloss: 0.122121\tvalid_1's auc: 0.7601\tvalid_1's binary_logloss: 0.139889\n",
      "[3]\ttraining's auc: 0.903389\ttraining's binary_logloss: 0.113871\tvalid_1's auc: 0.786056\tvalid_1's binary_logloss: 0.136631\n",
      "[4]\ttraining's auc: 0.915105\ttraining's binary_logloss: 0.1081\tvalid_1's auc: 0.784554\tvalid_1's binary_logloss: 0.135521\n",
      "[5]\ttraining's auc: 0.922881\ttraining's binary_logloss: 0.10283\tvalid_1's auc: 0.774704\tvalid_1's binary_logloss: 0.135758\n",
      "[6]\ttraining's auc: 0.927726\ttraining's binary_logloss: 0.098496\tvalid_1's auc: 0.783371\tvalid_1's binary_logloss: 0.134968\n",
      "[7]\ttraining's auc: 0.938258\ttraining's binary_logloss: 0.0945957\tvalid_1's auc: 0.783917\tvalid_1's binary_logloss: 0.134819\n",
      "[8]\ttraining's auc: 0.940359\ttraining's binary_logloss: 0.0910368\tvalid_1's auc: 0.791071\tvalid_1's binary_logloss: 0.133564\n",
      "[9]\ttraining's auc: 0.945975\ttraining's binary_logloss: 0.0877528\tvalid_1's auc: 0.796178\tvalid_1's binary_logloss: 0.132313\n",
      "[10]\ttraining's auc: 0.949547\ttraining's binary_logloss: 0.0846488\tvalid_1's auc: 0.795985\tvalid_1's binary_logloss: 0.131857\n",
      "[11]\ttraining's auc: 0.957079\ttraining's binary_logloss: 0.0818678\tvalid_1's auc: 0.801058\tvalid_1's binary_logloss: 0.131488\n",
      "[12]\ttraining's auc: 0.96227\ttraining's binary_logloss: 0.0789467\tvalid_1's auc: 0.798863\tvalid_1's binary_logloss: 0.130787\n",
      "[13]\ttraining's auc: 0.965094\ttraining's binary_logloss: 0.0763246\tvalid_1's auc: 0.799386\tvalid_1's binary_logloss: 0.130334\n",
      "[14]\ttraining's auc: 0.970599\ttraining's binary_logloss: 0.0735669\tvalid_1's auc: 0.804322\tvalid_1's binary_logloss: 0.129863\n",
      "[15]\ttraining's auc: 0.974814\ttraining's binary_logloss: 0.0710622\tvalid_1's auc: 0.791822\tvalid_1's binary_logloss: 0.129891\n",
      "[16]\ttraining's auc: 0.98107\ttraining's binary_logloss: 0.0684459\tvalid_1's auc: 0.795758\tvalid_1's binary_logloss: 0.129797\n",
      "[17]\ttraining's auc: 0.984432\ttraining's binary_logloss: 0.0660123\tvalid_1's auc: 0.793073\tvalid_1's binary_logloss: 0.130073\n",
      "[18]\ttraining's auc: 0.989215\ttraining's binary_logloss: 0.063898\tvalid_1's auc: 0.789957\tvalid_1's binary_logloss: 0.130329\n",
      "[19]\ttraining's auc: 0.991165\ttraining's binary_logloss: 0.0619877\tvalid_1's auc: 0.802286\tvalid_1's binary_logloss: 0.130239\n",
      "[20]\ttraining's auc: 0.991882\ttraining's binary_logloss: 0.0600792\tvalid_1's auc: 0.802286\tvalid_1's binary_logloss: 0.130162\n",
      "[21]\ttraining's auc: 0.994452\ttraining's binary_logloss: 0.0583566\tvalid_1's auc: 0.803606\tvalid_1's binary_logloss: 0.129955\n",
      "[22]\ttraining's auc: 0.99619\ttraining's binary_logloss: 0.0565168\tvalid_1's auc: 0.802832\tvalid_1's binary_logloss: 0.130223\n",
      "[23]\ttraining's auc: 0.996612\ttraining's binary_logloss: 0.0549096\tvalid_1's auc: 0.802605\tvalid_1's binary_logloss: 0.130319\n",
      "[24]\ttraining's auc: 0.997289\ttraining's binary_logloss: 0.0533691\tvalid_1's auc: 0.800466\tvalid_1's binary_logloss: 0.130755\n",
      "Early stopping, best iteration is:\n",
      "[14]\ttraining's auc: 0.970599\ttraining's binary_logloss: 0.0735669\tvalid_1's auc: 0.804322\tvalid_1's binary_logloss: 0.129863\n",
      "[1]\ttraining's auc: 0.847368\ttraining's binary_logloss: 0.13047\tvalid_1's auc: 0.790128\tvalid_1's binary_logloss: 0.148805\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's auc: 0.872362\ttraining's binary_logloss: 0.121091\tvalid_1's auc: 0.778877\tvalid_1's binary_logloss: 0.147345\n",
      "[3]\ttraining's auc: 0.891438\ttraining's binary_logloss: 0.113698\tvalid_1's auc: 0.799214\tvalid_1's binary_logloss: 0.144419\n",
      "[4]\ttraining's auc: 0.907929\ttraining's binary_logloss: 0.10763\tvalid_1's auc: 0.804345\tvalid_1's binary_logloss: 0.142138\n",
      "[5]\ttraining's auc: 0.914896\ttraining's binary_logloss: 0.102607\tvalid_1's auc: 0.808721\tvalid_1's binary_logloss: 0.14031\n",
      "[6]\ttraining's auc: 0.922427\ttraining's binary_logloss: 0.0979258\tvalid_1's auc: 0.802124\tvalid_1's binary_logloss: 0.138823\n",
      "[7]\ttraining's auc: 0.937851\ttraining's binary_logloss: 0.0938485\tvalid_1's auc: 0.804289\tvalid_1's binary_logloss: 0.137518\n",
      "[8]\ttraining's auc: 0.940167\ttraining's binary_logloss: 0.0901175\tvalid_1's auc: 0.817284\tvalid_1's binary_logloss: 0.137038\n",
      "[9]\ttraining's auc: 0.942015\ttraining's binary_logloss: 0.0870616\tvalid_1's auc: 0.820705\tvalid_1's binary_logloss: 0.136227\n",
      "[10]\ttraining's auc: 0.948165\ttraining's binary_logloss: 0.0840205\tvalid_1's auc: 0.816962\tvalid_1's binary_logloss: 0.137035\n",
      "[11]\ttraining's auc: 0.951909\ttraining's binary_logloss: 0.0810453\tvalid_1's auc: 0.802746\tvalid_1's binary_logloss: 0.136976\n",
      "[12]\ttraining's auc: 0.953426\ttraining's binary_logloss: 0.0785024\tvalid_1's auc: 0.811875\tvalid_1's binary_logloss: 0.137007\n",
      "[13]\ttraining's auc: 0.964984\ttraining's binary_logloss: 0.0759185\tvalid_1's auc: 0.804323\tvalid_1's binary_logloss: 0.136921\n",
      "[14]\ttraining's auc: 0.975516\ttraining's binary_logloss: 0.0731321\tvalid_1's auc: 0.80389\tvalid_1's binary_logloss: 0.136315\n",
      "[15]\ttraining's auc: 0.979865\ttraining's binary_logloss: 0.0708798\tvalid_1's auc: 0.799658\tvalid_1's binary_logloss: 0.136893\n",
      "[16]\ttraining's auc: 0.983411\ttraining's binary_logloss: 0.0685256\tvalid_1's auc: 0.810009\tvalid_1's binary_logloss: 0.136633\n",
      "[17]\ttraining's auc: 0.985744\ttraining's binary_logloss: 0.0661866\tvalid_1's auc: 0.813186\tvalid_1's binary_logloss: 0.13647\n",
      "[18]\ttraining's auc: 0.987182\ttraining's binary_logloss: 0.0644078\tvalid_1's auc: 0.814841\tvalid_1's binary_logloss: 0.136186\n",
      "[19]\ttraining's auc: 0.991102\ttraining's binary_logloss: 0.0622689\tvalid_1's auc: 0.816118\tvalid_1's binary_logloss: 0.135961\n",
      "Early stopping, best iteration is:\n",
      "[9]\ttraining's auc: 0.942015\ttraining's binary_logloss: 0.0870616\tvalid_1's auc: 0.820705\tvalid_1's binary_logloss: 0.136227\n",
      "[1]\ttraining's auc: 0.844382\ttraining's binary_logloss: 0.131654\tvalid_1's auc: 0.76197\tvalid_1's binary_logloss: 0.151806\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's auc: 0.872501\ttraining's binary_logloss: 0.12148\tvalid_1's auc: 0.754692\tvalid_1's binary_logloss: 0.1487\n",
      "[3]\ttraining's auc: 0.885858\ttraining's binary_logloss: 0.114131\tvalid_1's auc: 0.755757\tvalid_1's binary_logloss: 0.146429\n",
      "[4]\ttraining's auc: 0.896374\ttraining's binary_logloss: 0.108319\tvalid_1's auc: 0.753215\tvalid_1's binary_logloss: 0.145179\n",
      "[5]\ttraining's auc: 0.910251\ttraining's binary_logloss: 0.102995\tvalid_1's auc: 0.763697\tvalid_1's binary_logloss: 0.143977\n",
      "[6]\ttraining's auc: 0.922541\ttraining's binary_logloss: 0.0983664\tvalid_1's auc: 0.764175\tvalid_1's binary_logloss: 0.142805\n",
      "[7]\ttraining's auc: 0.932512\ttraining's binary_logloss: 0.0941559\tvalid_1's auc: 0.761807\tvalid_1's binary_logloss: 0.142723\n",
      "[8]\ttraining's auc: 0.942621\ttraining's binary_logloss: 0.0906036\tvalid_1's auc: 0.7626\tvalid_1's binary_logloss: 0.141607\n",
      "[9]\ttraining's auc: 0.948488\ttraining's binary_logloss: 0.0872799\tvalid_1's auc: 0.768161\tvalid_1's binary_logloss: 0.141534\n",
      "[10]\ttraining's auc: 0.958794\ttraining's binary_logloss: 0.0841266\tvalid_1's auc: 0.76373\tvalid_1's binary_logloss: 0.142147\n",
      "[11]\ttraining's auc: 0.96198\ttraining's binary_logloss: 0.0813083\tvalid_1's auc: 0.760775\tvalid_1's binary_logloss: 0.142514\n",
      "[12]\ttraining's auc: 0.963983\ttraining's binary_logloss: 0.0787126\tvalid_1's auc: 0.766358\tvalid_1's binary_logloss: 0.142359\n",
      "[13]\ttraining's auc: 0.971177\ttraining's binary_logloss: 0.0757679\tvalid_1's auc: 0.765554\tvalid_1's binary_logloss: 0.141883\n",
      "[14]\ttraining's auc: 0.97583\ttraining's binary_logloss: 0.0733076\tvalid_1's auc: 0.768911\tvalid_1's binary_logloss: 0.14178\n",
      "[15]\ttraining's auc: 0.978128\ttraining's binary_logloss: 0.0711031\tvalid_1's auc: 0.768998\tvalid_1's binary_logloss: 0.141972\n",
      "[16]\ttraining's auc: 0.980848\ttraining's binary_logloss: 0.0689229\tvalid_1's auc: 0.766456\tvalid_1's binary_logloss: 0.14246\n",
      "[17]\ttraining's auc: 0.985486\ttraining's binary_logloss: 0.0668628\tvalid_1's auc: 0.763979\tvalid_1's binary_logloss: 0.143094\n",
      "[18]\ttraining's auc: 0.988606\ttraining's binary_logloss: 0.0648545\tvalid_1's auc: 0.758505\tvalid_1's binary_logloss: 0.143889\n",
      "[19]\ttraining's auc: 0.990324\ttraining's binary_logloss: 0.0627044\tvalid_1's auc: 0.764457\tvalid_1's binary_logloss: 0.143046\n",
      "Early stopping, best iteration is:\n",
      "[9]\ttraining's auc: 0.948488\ttraining's binary_logloss: 0.0872799\tvalid_1's auc: 0.768161\tvalid_1's binary_logloss: 0.141534\n",
      "[1]\ttraining's auc: 0.833479\ttraining's binary_logloss: 0.134773\tvalid_1's auc: 0.666217\tvalid_1's binary_logloss: 0.115007\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's auc: 0.872636\ttraining's binary_logloss: 0.125355\tvalid_1's auc: 0.732873\tvalid_1's binary_logloss: 0.112855\n",
      "[3]\ttraining's auc: 0.89442\ttraining's binary_logloss: 0.117811\tvalid_1's auc: 0.728139\tvalid_1's binary_logloss: 0.111632\n",
      "[4]\ttraining's auc: 0.901312\ttraining's binary_logloss: 0.111678\tvalid_1's auc: 0.746241\tvalid_1's binary_logloss: 0.110578\n",
      "[5]\ttraining's auc: 0.916054\ttraining's binary_logloss: 0.106465\tvalid_1's auc: 0.750492\tvalid_1's binary_logloss: 0.109793\n",
      "[6]\ttraining's auc: 0.921989\ttraining's binary_logloss: 0.101893\tvalid_1's auc: 0.757542\tvalid_1's binary_logloss: 0.108572\n",
      "[7]\ttraining's auc: 0.930591\ttraining's binary_logloss: 0.0979\tvalid_1's auc: 0.760259\tvalid_1's binary_logloss: 0.107814\n",
      "[8]\ttraining's auc: 0.944368\ttraining's binary_logloss: 0.0941002\tvalid_1's auc: 0.763326\tvalid_1's binary_logloss: 0.107352\n",
      "[9]\ttraining's auc: 0.946913\ttraining's binary_logloss: 0.0906436\tvalid_1's auc: 0.771994\tvalid_1's binary_logloss: 0.106646\n",
      "[10]\ttraining's auc: 0.950146\ttraining's binary_logloss: 0.0875688\tvalid_1's auc: 0.771444\tvalid_1's binary_logloss: 0.105966\n",
      "[11]\ttraining's auc: 0.955785\ttraining's binary_logloss: 0.0846147\tvalid_1's auc: 0.767177\tvalid_1's binary_logloss: 0.106213\n",
      "[12]\ttraining's auc: 0.961688\ttraining's binary_logloss: 0.0817208\tvalid_1's auc: 0.768494\tvalid_1's binary_logloss: 0.106133\n",
      "[13]\ttraining's auc: 0.964739\ttraining's binary_logloss: 0.079093\tvalid_1's auc: 0.76806\tvalid_1's binary_logloss: 0.105939\n",
      "[14]\ttraining's auc: 0.974089\ttraining's binary_logloss: 0.0761215\tvalid_1's auc: 0.775511\tvalid_1's binary_logloss: 0.105451\n",
      "[15]\ttraining's auc: 0.98048\ttraining's binary_logloss: 0.0734196\tvalid_1's auc: 0.773377\tvalid_1's binary_logloss: 0.104816\n",
      "[16]\ttraining's auc: 0.983542\ttraining's binary_logloss: 0.0710244\tvalid_1's auc: 0.766793\tvalid_1's binary_logloss: 0.10477\n",
      "[17]\ttraining's auc: 0.986263\ttraining's binary_logloss: 0.0687868\tvalid_1's auc: 0.763043\tvalid_1's binary_logloss: 0.105013\n",
      "[18]\ttraining's auc: 0.987468\ttraining's binary_logloss: 0.0667622\tvalid_1's auc: 0.765177\tvalid_1's binary_logloss: 0.104658\n",
      "[19]\ttraining's auc: 0.988201\ttraining's binary_logloss: 0.0649486\tvalid_1's auc: 0.764676\tvalid_1's binary_logloss: 0.104628\n",
      "[20]\ttraining's auc: 0.988917\ttraining's binary_logloss: 0.0630806\tvalid_1's auc: 0.763443\tvalid_1's binary_logloss: 0.104629\n",
      "[21]\ttraining's auc: 0.99336\ttraining's binary_logloss: 0.0611844\tvalid_1's auc: 0.760543\tvalid_1's binary_logloss: 0.10449\n",
      "[22]\ttraining's auc: 0.99493\ttraining's binary_logloss: 0.0595925\tvalid_1's auc: 0.760976\tvalid_1's binary_logloss: 0.104559\n",
      "[23]\ttraining's auc: 0.996111\ttraining's binary_logloss: 0.0577829\tvalid_1's auc: 0.758693\tvalid_1's binary_logloss: 0.104376\n",
      "[24]\ttraining's auc: 0.996856\ttraining's binary_logloss: 0.0564855\tvalid_1's auc: 0.760776\tvalid_1's binary_logloss: 0.104374\n",
      "Early stopping, best iteration is:\n",
      "[14]\ttraining's auc: 0.974089\ttraining's binary_logloss: 0.0761215\tvalid_1's auc: 0.775511\tvalid_1's binary_logloss: 0.105451\n",
      "[1]\ttraining's auc: 0.825297\ttraining's binary_logloss: 0.130534\tvalid_1's auc: 0.761685\tvalid_1's binary_logloss: 0.158716\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's auc: 0.849181\ttraining's binary_logloss: 0.12157\tvalid_1's auc: 0.753543\tvalid_1's binary_logloss: 0.156192\n",
      "[3]\ttraining's auc: 0.886893\ttraining's binary_logloss: 0.114101\tvalid_1's auc: 0.798973\tvalid_1's binary_logloss: 0.151368\n",
      "[4]\ttraining's auc: 0.90482\ttraining's binary_logloss: 0.107786\tvalid_1's auc: 0.804686\tvalid_1's binary_logloss: 0.149782\n",
      "[5]\ttraining's auc: 0.918629\ttraining's binary_logloss: 0.10279\tvalid_1's auc: 0.816804\tvalid_1's binary_logloss: 0.147968\n",
      "[6]\ttraining's auc: 0.922879\ttraining's binary_logloss: 0.0985091\tvalid_1's auc: 0.826177\tvalid_1's binary_logloss: 0.145362\n",
      "[7]\ttraining's auc: 0.928425\ttraining's binary_logloss: 0.0942206\tvalid_1's auc: 0.828972\tvalid_1's binary_logloss: 0.143577\n",
      "[8]\ttraining's auc: 0.939508\ttraining's binary_logloss: 0.0907756\tvalid_1's auc: 0.828352\tvalid_1's binary_logloss: 0.142684\n",
      "[9]\ttraining's auc: 0.94208\ttraining's binary_logloss: 0.0874606\tvalid_1's auc: 0.828322\tvalid_1's binary_logloss: 0.142309\n",
      "[10]\ttraining's auc: 0.951182\ttraining's binary_logloss: 0.0842737\tvalid_1's auc: 0.825313\tvalid_1's binary_logloss: 0.141885\n",
      "[11]\ttraining's auc: 0.956367\ttraining's binary_logloss: 0.0814271\tvalid_1's auc: 0.823839\tvalid_1's binary_logloss: 0.141952\n",
      "[12]\ttraining's auc: 0.960299\ttraining's binary_logloss: 0.0788711\tvalid_1's auc: 0.841303\tvalid_1's binary_logloss: 0.141003\n",
      "[13]\ttraining's auc: 0.964784\ttraining's binary_logloss: 0.0761346\tvalid_1's auc: 0.845725\tvalid_1's binary_logloss: 0.139258\n",
      "[14]\ttraining's auc: 0.972843\ttraining's binary_logloss: 0.0735397\tvalid_1's auc: 0.842909\tvalid_1's binary_logloss: 0.138874\n",
      "[15]\ttraining's auc: 0.978963\ttraining's binary_logloss: 0.0710237\tvalid_1's auc: 0.838863\tvalid_1's binary_logloss: 0.138848\n",
      "[16]\ttraining's auc: 0.982351\ttraining's binary_logloss: 0.068807\tvalid_1's auc: 0.838619\tvalid_1's binary_logloss: 0.138572\n",
      "[17]\ttraining's auc: 0.986283\ttraining's binary_logloss: 0.0665443\tvalid_1's auc: 0.83372\tvalid_1's binary_logloss: 0.138893\n",
      "[18]\ttraining's auc: 0.989689\ttraining's binary_logloss: 0.0643537\tvalid_1's auc: 0.830782\tvalid_1's binary_logloss: 0.139185\n",
      "[19]\ttraining's auc: 0.992155\ttraining's binary_logloss: 0.0623117\tvalid_1's auc: 0.835895\tvalid_1's binary_logloss: 0.138295\n",
      "[20]\ttraining's auc: 0.992648\ttraining's binary_logloss: 0.0606387\tvalid_1's auc: 0.837969\tvalid_1's binary_logloss: 0.138148\n",
      "[21]\ttraining's auc: 0.994065\ttraining's binary_logloss: 0.0589597\tvalid_1's auc: 0.835732\tvalid_1's binary_logloss: 0.138418\n",
      "[22]\ttraining's auc: 0.994727\ttraining's binary_logloss: 0.0574872\tvalid_1's auc: 0.83805\tvalid_1's binary_logloss: 0.138307\n",
      "[23]\ttraining's auc: 0.995462\ttraining's binary_logloss: 0.0558451\tvalid_1's auc: 0.839026\tvalid_1's binary_logloss: 0.138296\n",
      "Early stopping, best iteration is:\n",
      "[13]\ttraining's auc: 0.964784\ttraining's binary_logloss: 0.0761346\tvalid_1's auc: 0.845725\tvalid_1's binary_logloss: 0.139258\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "fi = []\n",
    "test_probs = []\n",
    "i = 0\n",
    "for train_idx, valid_idx in model_selection.KFold(n_splits=10, shuffle=True).split(X_train):\n",
    "    i += 1\n",
    "    Xt = X_train.iloc[train_idx]\n",
    "    yt = y_train.loc[X_train.index].iloc[train_idx]\n",
    "\n",
    "    Xv = X_train.iloc[valid_idx]\n",
    "    yv = y_train.loc[X_train.index].iloc[valid_idx]\n",
    "\n",
    "    learner = LGBMClassifier(n_estimators=10000)\n",
    "    learner.fit(Xt, yt,  early_stopping_rounds=10, eval_metric=\"auc\",\n",
    "                eval_set=[(Xt, yt), (Xv, yv)])\n",
    "    \n",
    "    test_probs.append(pd.Series(learner.predict_proba(X_test)[:, -1],\n",
    "                                index=X_test.index, name=\"fold_\" + str(i)))\n",
    "    fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "\n",
    "test_probs = pd.concat(test_probs, axis=1).mean(axis=1)\n",
    "test_probs.index.name=\"USER_ID\"\n",
    "test_probs.name=\"SCORE\"\n",
    "test_probs.to_csv(\"benchmark.zip\", header=True, compression=\"zip\")\n",
    "fi = pd.concat(fi, axis=1).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
